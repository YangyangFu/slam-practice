{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Camera and Image\n",
    "\n",
    "Goal of study:\n",
    "- learn the models of pinhole camera, instrinsic, extrinsic and distortion\n",
    "- learn how to project a spatial point to an image 2-D plane\n",
    "- learn the basic image processing techniques in OpenCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part aims to introduce \"how the robotics observe the world\". The camera is the most important sensor in robotics. The camera can observe the world and provide the information to the robot. The camera can be used to detect the object, to measure the distance, to track the motion, to recognize the scene, to localize the robot, to map the environment, etc. \n",
    "\n",
    "The camera is a device that can capture the light and convert it to the electrical signal. The camera can be used to capture the image of the world. The image is a 2-D array of pixels. Each pixel is a small square that can be used to represent the color of the light. The color of the light can be represented by the RGB (Red, Green, Blue) color space. The RGB color space is a 3-D space that can be used to represent the color of the light. The RGB color space is a very popular color space in computer vision.\n",
    "\n",
    "**How does a camera work?**\n",
    "\n",
    "When a camera captures an image, it records the light information from a scene and converts it into a digital representation that can be processed and displayed by computers or other electronic devices. Here's a step-by-step explanation of how an image is generated by a digital camera:\n",
    "\n",
    "- `Light enters the camera`: When you point a camera at a scene, the light reflected from the objects in the scene enters the camera through the lens, which is designed to focus and control the light.\n",
    "- `Focused light`: The lens focuses the incoming light onto the camera's image sensor, a light-sensitive semiconductor device typically made of charge-coupled device (CCD) or complementary metal-oxide-semiconductor (CMOS) technology.\n",
    "- `Image sensor`: The image sensor consists of millions of tiny light-sensitive elements called pixels (short for \"picture elements\"). Each pixel detects the intensity of the light that falls on it, and in the case of color cameras, a color filter array (such as the Bayer filter) is used to record color information. The filter array has a pattern of red, green, and blue filters, which allows each pixel to detect a specific color.\n",
    "- `Analog-to-digital conversion`: The light intensity and color information detected by the pixels are initially in the form of analog signals. These analog signals are then converted into digital signals by an analog-to-digital converter (ADC). Each pixel's intensity value is represented as a number, usually in an 8-bit, 10-bit, or higher format. This means that, for example, an 8-bit image can represent 256 different intensity levels for each color channel.\n",
    "- `Demosaicing`: Since each pixel only detects one color (red, green, or blue) due to the color filter array, a process called demosaicing is used to reconstruct the full-color image. Demosaicing algorithms estimate the missing color information for each pixel by considering the values of neighboring pixels. This results in an image where each pixel has intensity values for red, green, and blue channels.\n",
    "- `Image processing`: The camera's image processor takes the raw data from the sensor and applies various adjustments and corrections, such as white balance, color correction, contrast enhancement, and noise reduction. This processing helps create a more visually pleasing and accurate representation of the scene.\n",
    "- `Image compression and storage`: The processed image is typically compressed using a format like JPEG to reduce file size and make it easier to store and share. The compressed image is then saved to the camera's internal memory or a removable storage device like an SD card.\n",
    "\n",
    "The final digital image generated by the camera is a matrix of pixels, where each pixel has intensity values for red, green, and blue color channels. This image can be displayed on a screen, further processed by computer vision algorithms, or printed on paper."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Camera Mathematical Model\n",
    "\n",
    "### 5.1.1 Pinhole Camera Model\n",
    "\n",
    "The camera model is to describe the transformation of an object in the 3-D workd to its 2-D image.\n",
    "Say a 3-D point $P_w(x_w, y_w, z_w)$ in the world coordinate system is projected to a 2-D point $p(u,v)$ in the image plane by a camera of pose $T$. \n",
    "How to get $p(u,v)$ in the 2-D image?\n",
    "\n",
    "\n",
    "- image plane: the 2-D plane that the image is projected to\n",
    "\n",
    "Say the 3-D point in the camera coordinate system is $P_c(x_c, y_c, z_c)$. \n",
    "\n",
    "$$\n",
    "P_c = T \\cdot P_w\n",
    "$$\n",
    "\n",
    "The image of $P_c$ in the image plane is $p(u,v)$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(u,v,1) &= \\frac{1}{z_c} \\cdot K \\cdot T \\cdot P_w\\\\\n",
    "&= \\frac{1}{z_c}\\begin{bmatrix} \n",
    "        f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \n",
    "    \\end{bmatrix} \n",
    "    \\cdot \n",
    "    \\begin{bmatrix} \n",
    "        R_{3 \\times 3} & t_{3 \\times 1} \\\\ 0 & 1 \n",
    "    \\end{bmatrix} \n",
    "    \\cdot \n",
    "    \\begin{bmatrix} \n",
    "        x_w \\\\ y_w \\\\ z_w  \n",
    "    \\end{bmatrix} \\\\\n",
    "&= \\frac{1}{z_c} \\cdot K \\cdot P_c \\\\\n",
    "& = K \\cdot \n",
    "    \\begin{bmatrix}\n",
    "        \\frac{x_c}{z_c} \\\\\n",
    "        \\frac{y_c}{z_c} \\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The camera pose $R$ and $t$ are known as camera extrinsics, which are the parameters that describe the position and orientation of the camera relative to the world coordinate system. \n",
    "Since usually the camera moves with the mobile robotics, the camera extrinsics are usually unknown, and need to be estimated from the image data, which is a major part of the SLAM.\n",
    "\n",
    "The projection process can also be viewed as: we can convert a world coordinate point to the camera coordinate system first and then remove the last dimension. The depth of the point from the imaging plane of the camera is then removed, which is equivalent to the normalization on the last dimension. In this way, we get the projection of the point $P$ on the camera \\textit{normalized plane}:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Distortion Model\n",
    "We typically add lens in front of the camera to focus and control the light. The lens can introduce distortion to the image. The distortion can be divided into two categories: radial distortion and tangential distortion.\n",
    "\n",
    "The distortion model is to correct the pixel coordinates in the camera normalized plane $P_{cn}(x,y,1) = P_{cn}(\\frac{x_c}{z_c}, \\frac{y_c}{z_c}, 1)$.\n",
    "\n",
    "For radial distortion, we can use an empirical model to describe the distortion. \n",
    "We define the radius of $r$ as $\\sqrt {x^2 + y^2}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    x_d &= x(1+k_1r^2+k_2r^4+k_3r^6)\\\\\n",
    "    y_d &= y(1+k_1r^2+k_2r^4+k_3r^6)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For tangential distortion, we can add two more parameters $p_1$ and $p_2$ to describe:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    x_d &= x + 2p_1xy+p_2(r^2+2x^2)\\\\\n",
    "    y_d &= y + p_1(r^2+2y^2)+2p_2xy\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, by introducing additional 5 parameters, we can find the correct pixel coordinates in the image plane.\n",
    "\n",
    "1. project the 3-D point to the camera normalized plane, and its coordinates is $[x,y]$\n",
    "2. apply the distortion model to the coordinates, and get the distorted coordinates $[x_d, y_d]$ by:\n",
    "$$\n",
    "\\begin{align}\n",
    "    x_d &= x(1+k_1r^2+k_2r^4+k_3r^6) + 2p_1xy+p_2(r^2+2x^2)\\\\\n",
    "    y_d &= y(1+k_1r^2+k_2r^4+k_3r^6) + p_1(r^2+2y^2)+2p_2xy\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "3. convert the distorted coordinates to the pixel coordinates in the image plane, and get the pixel coordinates $[u,v]$ by:\n",
    "$$\n",
    "\\begin{align}\n",
    "    u &= f_x x_d + c_x\\\\\n",
    "    v &= f_y y_d + c_y\\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "Typical workflow is to undistort the image first and then discuss the coordinate in the image plane. \n",
    "This does not need add distortion model to the camera mdoel."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
